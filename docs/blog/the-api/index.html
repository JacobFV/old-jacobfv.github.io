<!DOCTYPE html> <html lang=""> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jacob Valdez | The API</title> <meta name="author" content="Jacob F. Valdez"/> <meta name="description" content="The anthrocentric policy interface"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jacobfv.github.io/blog/the-api/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Jacob Valdez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/resume/">resume</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/papers/">papers</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://jacobfvaldez.weebly.com/" target="_blank" rel="noopener noreferrer">high school</a> </div> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">The API</h1> <p class="post-meta">October 18, 2021</p> <p class="post-tags"> <a href="/blog/2021"> <i class="fas fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/agi"> <i class="fas fa-hashtag fa-sm"></i> agi</a>     ·   <a href="/blog/category/ideas"> <i class="fas fa-tag fa-sm"></i> ideas</a>   </p> </header> <article class="post-content"> <p>Though diverse, most humans share a common set of input and output modalities: sight, hearing, touch, and skeletal motor control. This common interface aligns our perception of the world to a rasonable degree and facilitates social interaction and collaboration. As progress continues towards developing humanly-impactful foundation models and policies, I propose establishing a common set of modalities for artificial intelligence systems as well: the <strong>anthrocentric policy interface</strong> or <strong>API</strong>. The API is not a formal application program interface specification but rather a description of the modalities that can be used to interact with an AI system. Following is my proposal for momdalities included by version 0.0 of the API:</p> <ul> <li>motor: sparse variable-dimensional graph of continuous motor values (\(x_{motor}, y_{motor} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to produce appropriate outputs as modulated by their reward signals.</li> <li>touch: sparse variable-dimensional graph of continuous touch values (\(x_{touch}, y_{touch} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to output predicted touch values for the next interaction step.</li> <li>reward: single or multidimensional reward signal (vector, \(x_{reward}, y_{reward} \in \mathbb{R}^n, n \in \mathbb{N}\)). Sparse or dense. API-compatible models are expected to output predicted reward value for the next interaction step.</li> <li>text: variable length string of tokens (\(x_{text}, y_{text} \in \mathbb{Z}^t, t \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted text values for the next interaction step.</li> <li>audio: variable length, single or multi-channel waveform (\(x_{audio}, y_{audio} \in [0,1]^{t,c}, t,c \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted audio values for the next interaction step.</li> <li>video: variable length, variable size, variable channel image sequence (\(x_{video}, y_{video} \in [0,1]^{t,h,w,c}, t \in \mathbb{Z}_{+}, h,w,c \in \mathbb{N}\)). API-compatible models are expected to output predicted video values for the next interaction step.</li> <li>hidden state: arbitrary data structure. Developers can use this to store and retrieve state information between interactions.</li> </ul> <p>All modalities exchange input and output information over multiple interaction steps. For example, a vision transformer may recieve top down feedback and attend for multiple steps over a single image, an in-context learning model might recieve several batches of dataset examples over the interaction, or a conversation bot could be force fit on real conversations over time. This demands clarify time:</p> <ul> <li>training step = number of dataset minibatches or environment episodes trained on</li> <li>interaction step = number of modality inputs and outputs performed; the classical time measure within a single episode</li> <li>modality time step = which frame number in the video sequence; which token index in the text sequence</li> </ul> <p>From the client’s perspective, modality inputs and outputs are optional. These will be represented by <code class="language-plaintext highlighter-rouge">None</code> in Python, <code class="language-plaintext highlighter-rouge">null</code> in JSON, or similar in other languages. For example, you might only occasionally send images to a multimodal conversation bot but utilize the text modality on every step. Some ‘multimodal’ datasets and environments will only provide information for two or three modalities. Additionally, a transcribed audiovideo dataset might have zero-length text sequences, waveforms, and videos. Not all outputs will be used in every application so they can be explicitly marked as <code class="language-plaintext highlighter-rouge">do_not_compute</code>.</p> <p>Handling time steps within each modality introduces many technical challanges. Developer must consider questions as: how should a multimodal video model combine 8KHz audio with 12 FPS video? will the robot controller be presented with 100ms, 6-frame snippets of 60FPS video on each interaction step? or should it just recieve a single frame every 50ms? Should an audio transcriber recieve the full audio track or just 10 second staggered segments on each interaction step? Or should it use its motor modality to deliberately move its perception window around?</p> <p>API-compatible models are expected to accommadate arbitrary numbers of each modality at initialization time. For example, you might initialize a policy with <code class="language-plaintext highlighter-rouge">vision:left</code> and <code class="language-plaintext highlighter-rouge">vision:right</code> or <code class="language-plaintext highlighter-rouge">text:magent</code> (social communication) and <code class="language-plaintext highlighter-rouge">text:control</code> (user directions). However, API-compatible models are not required to accomodate new modalities after initialization (you don’t have to tie the weights; the <code class="language-plaintext highlighter-rouge">text:magent</code> preprocessor network can be very different from <code class="language-plaintext highlighter-rouge">text:control</code>)</p> <p>I imagine a few ways this interface may be used:</p> <ul> <li>For N-way classification problems, you might have a motor graph with N 1-dimensional nodes.</li> <li>For N-dimensional regression, you might have a motor graph with just 1 N-dimensional node.</li> <li>For computer vision, the model iteratively attends to the image.</li> <li>For 3D robotic agents, … You get the idea.</li> <li>Managers might ask, ‘Is it compatible with the API?’</li> <li>Researchers might write, ‘this dataset/environment conforms to the API’.</li> </ul> <p>I hope the API provides a common interface to facilitate community collaboration towards developing increasingly strong and general artificial intelligence systems. My focus for the next few months will be building the Artificial Experience – a dataset of datasets and environments (including procedurally generated ones) that can provide training signals for increasingly general API-compatible agents.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2022 Jacob F. Valdez. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams'
        }
      };
    </script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>
