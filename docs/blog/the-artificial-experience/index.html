<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Jacob Valdez


  | The Artificial Experience

</title>
<meta name="description" content="Personal R&D portfolio site
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’»</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/the-artificial-experience/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       Jacob Valdez
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/resume/">
                resume
                
              </a>
          </li>
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">The Artificial Experience</h1>
    <p class="post-meta">October 10, 2021</p>
  </header>

  <article class="post-content">
    <p><strong>Update: the Artificial Experience is under work at <a href="https://github.com/JacobFV/artificial-experience">https://github.com/JacobFV/artificial-experience</a></strong></p>

<p>Our implicit objective in the hypothetical artificial general intelligence is to identify as many dimensions of variation to the underlying data structures that real Intelligence operates on and iterate development around that data. For datasets includes:</p>
<ul>
  <li>domain: natural language, vision, audio, robot, etc.</li>
  <li>data structure: structured, text, image, video, audio, graph, etc., multimodal</li>
  <li>data representation: discrete, continuous, categorical, binary, etc.</li>
  <li>problem: classification, regression, clustering, autoencoding, autoregression, etc., no specified problem type.</li>
  <li>data augmentations.</li>
</ul>

<p>For environments we might consider:</p>
<ul>
  <li>simulated/real</li>
  <li>data representation: discrete, continuous, categorical, binary, etc.</li>
  <li>single objective/multi-objective/no-objective</li>
  <li>partially/fully observable</li>
  <li>markovian/non-markovian</li>
  <li>single agent/multi-agent</li>
  <li>for multi-agent: cooperative/competitive/mixed-mode</li>
</ul>

<p>Iâ€™ve listed several datasets and environments in the bottom of this post. Ideally, we should train increasingly general ML systems over all of these variations. Still, our training pipelines are very brittle.</p>

<p>I propose developing a tool that allows ML praticioners to easily train their agents across many datasets and environments: the Artificial Experience (<code class="language-plaintext highlighter-rouge">ae</code>). <code class="language-plaintext highlighter-rouge">ae</code> should provide minially necesary extensions to extend existing open-source dataset loaders, environments, and hubs. It should be agnostic to the actual training paradigm and tricks (augmentations, experience replay, cirriculum learning, etc.) but itegrate cleanly with tools that do. The following is a declarative description of what I plan to make:</p>

<p>The ArtificialExperience environment (<code class="language-plaintext highlighter-rouge">AEEnv</code>) provides a wrapper for multiple environments. Datasets may be wrapped into environments. Turn-based multiagent environments are wrapped into parallel agent cycles environments (you can unwrap this later in your multiagent executor). An AEEnv might look like this:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">AEEnv</span><span class="p">(</span><span class="n">envs</span><span class="o">=</span><span class="p">[</span>
    <span class="n">DatasetEnv</span><span class="p">(</span><span class="n">tfds</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'coco'</span><span class="p">)),</span> <span class="c1"># multimodal information
</span>    <span class="n">DatasetEnv</span><span class="p">(</span><span class="n">hub</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'hub://activeloop/mnist-train'</span><span class="p">))</span>  <span class="c1"># cloud-native data
</span>    <span class="n">DatasetEnv</span><span class="p">(</span><span class="n">tfds</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'anli'</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">),</span> <span class="c1"># quick customization
</span>    <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">),</span> <span class="c1"># continous observation, discrete control
</span>    <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'Pong-v0'</span><span class="p">),</span> <span class="c1"># rgb image, discrete actions
</span>    <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'HalfCheetah-v2'</span><span class="p">),</span> <span class="c1"># continuous observation, continuous control
</span>    <span class="n">gym_starcraft</span><span class="p">.</span><span class="n">envs</span><span class="p">.</span><span class="n">starcraft_base_env</span><span class="p">(),</span> <span class="c1"># starcraft env
</span>    <span class="n">pettingzoo</span><span class="p">.</span><span class="n">atari</span><span class="p">.</span><span class="n">mario_bros_v2</span><span class="p">.</span><span class="n">env</span><span class="p">()</span> <span class="c1"># multiagent atari env
</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">AEEnv</code> also makes it easy to train on prespecified problem domains with datasets and environments minimally specified by some overlapping hierarchial tag-based system. Not all environments have the <code class="language-plaintext highlighter-rouge">.tag</code> attribute, so those will be ignored. However, the inbuilt list of envionrments should all support this schema. These filters can be changed at any moment between <code class="language-plaintext highlighter-rouge">AEEnv</code> steps. See Appendix A for a list of what I want to support.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">AEEnv</span><span class="p">(</span>
    <span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s">'domain:text-commonsense'</span><span class="p">,</span> <span class="s">'domain:image'</span><span class="p">,</span> <span class="s">'domain:multiagent'</span><span class="p">],</span>
    <span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s">'domain:reward-free-rl'</span><span class="p">,</span> <span class="s">'domain:multiagent/atari'</span><span class="p">,</span> <span class="s">'test:True'</span><span class="p">],</span>
<span class="p">)</span> <span class="c1"># train on text-commonsense (specific), image datasets (broad), and multiagent RL environments (broad) but don't train on the multiagent/atari environment or multiagent environments that don't have a environment specified reward.
</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">AEEnv</span><span class="p">()</span> <span class="c1"># train on all inbuilt datasets and environments
</span></code></pre></div></div>

<p>A <code class="language-plaintext highlighter-rouge">next_env_fn(last_step_data: step, curr_env: env, available_envs: List[env]) -&gt; env</code> determines which environment to sample from at <em>each</em> timestep. This may be a simple â€˜wait until all doneâ€™s are trueâ€™ (for datasets, after all epochs) or it may be a more complex user-designed autocirricula system. An <code class="language-plaintext highlighter-rouge">env_transition_fn(old_env: env, new_env: env) -&gt; NoReturn</code> can be specified to make surface-level model changes when the environment (and hence its interface) changes.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># samples a different environment *at every step*. Simple way to train on a diverse lot of datasets within the same problem domain (like images).
</span><span class="n">next_env_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">last_step_data</span><span class="p">,</span> <span class="n">curr_env</span><span class="p">,</span> <span class="n">available_envs</span><span class="p">:</span> <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">available_envs</span><span class="p">)</span> 
<span class="n">env</span><span class="p">.</span><span class="n">next_env_fn</span> <span class="o">=</span> <span class="n">next_env_fn</span>  <span class="c1"># lazy next_env_fn specification
</span>
<span class="c1"># samples a different environment *after every epoch*. Traditional approach to multi-dataset training.
</span><span class="k">def</span> <span class="nf">next_env_fn</span><span class="p">(</span><span class="n">last_step_data</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="n">curr_env</span><span class="p">:</span> <span class="n">env</span><span class="p">,</span> <span class="n">available_envs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">env</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">env</span><span class="p">:</span> 
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">last_step_data</span><span class="p">[</span><span class="n">agent_name</span><span class="p">].</span><span class="n">done</span> <span class="k">for</span> <span class="n">agent_name</span> <span class="ow">in</span> <span class="n">curr_env</span><span class="p">.</span><span class="n">agent_names</span><span class="p">):</span>
    <span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">available_envs</span><span class="p">)</span> 
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">curr_env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">AEEnv</span><span class="p">(...,</span> <span class="n">next_env_fn</span><span class="o">=</span><span class="n">next_env_fn</span><span class="p">)</span>  <span class="c1"># early next_env_fn specification
</span>
<span class="c1"># builds a new input and output layer for new environments
</span><span class="k">def</span> <span class="nf">env_transition_fn</span><span class="p">(</span><span class="n">old_env</span><span class="p">:</span> <span class="n">env</span><span class="p">,</span> <span class="n">new_env</span><span class="p">:</span> <span class="n">env</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">ae</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">nest</span><span class="p">.</span><span class="nb">all</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="o">==</span><span class="n">y</span><span class="p">,</span> 
        <span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">old_env</span><span class="p">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">old_env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">],</span>
        <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">new_env</span><span class="p">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">new_env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">]):</span>
        <span class="c1"># the environments are compatible, no need to change the model
</span>        <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># the environments are incompatible, we need to change the model
</span>        <span class="n">build_new_input_layer</span><span class="p">(</span><span class="n">new_env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">)</span>
        <span class="n">build_new_output_layer</span><span class="p">(</span><span class="n">new_env</span><span class="p">.</span><span class="n">action_space</span><span class="p">)</span>
        <span class="k">return</span>
<span class="n">env</span><span class="p">.</span><span class="n">env_transition_fn</span> <span class="o">=</span> <span class="n">env_transition_fn</span>  <span class="c1"># lazy env_transition_fn specification
</span>
<span class="c1"># builds a new input and output layer for new environments
</span><span class="k">def</span> <span class="nf">env_transition_fn</span><span class="p">(</span><span class="n">old_env</span><span class="p">:</span> <span class="n">env</span><span class="p">,</span> <span class="n">new_env</span><span class="p">:</span> <span class="n">env</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NoReturn</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">old_env</span><span class="p">,</span> <span class="n">DatasetEnv</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_env</span><span class="p">,</span> <span class="n">DatasetEnv</span><span class="p">):</span>
        <span class="c1"># the environments are either both datasets or both regular environments, no need to change the model
</span>        <span class="k">return</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># the environments are incompatible, we need to change the training pipeline
</span>        <span class="n">change_training_pipeline</span><span class="p">(</span><span class="n">new_env</span><span class="p">)</span>
        <span class="k">return</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">AEEnv</span><span class="p">(...,</span> <span class="n">env_transition_fn</span><span class="o">=</span><span class="n">env_transition_fn</span><span class="p">)</span>  <span class="c1"># early env_transition_fn specification
</span></code></pre></div></div>

<p>Data is presented at each step as an agent-separated dictionary of namedtuple <code class="language-plaintext highlighter-rouge">Step</code>â€™s as well as meta information about the environment state (or dataset index). A <code class="language-plaintext highlighter-rouge">Step</code> is a nested batch of <code class="language-plaintext highlighter-rouge">observation</code>, <code class="language-plaintext highlighter-rouge">reward</code>, <code class="language-plaintext highlighter-rouge">done</code>, and <code class="language-plaintext highlighter-rouge">information</code> . In most cases, these fields will be <code class="language-plaintext highlighter-rouge">None</code>. For example, datasets do not provide a reward (reward is determined by the training pipeline which is not part of <code class="language-plaintext highlighter-rouge">AEEnv</code>).For supervised learning datasets, the observations include both X and Y while for unsupervised learning datasets, the observations include only X. Also, most datasets and environments will only present information for a single agent. Here are some examples:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">evalLoop</span><span class="p">(</span><span class="n">agents</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">env</span><span class="p">.</span><span class="n">is_supervised</span><span class="p">)</span>

    <span class="n">step</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">last_step_data</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">done</span> <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="n">agents</span><span class="p">):</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">agent_name</span><span class="p">:</span> <span class="n">agent</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent_name</span><span class="p">].</span><span class="n">observation</span><span class="p">)</span> 
            <span class="k">for</span> <span class="n">agent_name</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="n">agent_names</span>
        <span class="p">}</span>

        <span class="n">step</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">env</span><span class="p">.</span><span class="n">agent_names</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">observation</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">reward</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">done</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">step</span><span class="p">[</span><span class="n">agent</span><span class="p">].</span><span class="n">information</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="appendix-a-datasets-and-environments">Appendix A: Datasets and environments</h2>

<p>The categories overlap. For instance, image captioning might be in the <code class="language-plaintext highlighter-rouge">image</code> category, but also in the <code class="language-plaintext highlighter-rouge">text</code> category. The high-level hierarchy might be:</p>
<ul>
  <li>images</li>
  <li>text</li>
  <li>video</li>
  <li>audio
TODO</li>
</ul>

<h3 id="nlp">NLP</h3>
<p>from Googleâ€™s <a href="https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html">FLAN blog post</a>:</p>
<ul>
  <li>Natural language inference: ANLI, RTE, CB, SNLI, MNLI, QNLI, WNLI, QNLI,</li>
  <li>Commonsense: CoPA, HeliaSwag, PiQA, StoryCloze</li>
  <li>Sentiment: IMDB, Sent140, SST-2, Yelp</li>
  <li>Paraphrase: MRPC, QQP, PAWS, STS-B</li>
  <li>Closed book QA: ARC (easy/chal), NQ, TQA</li>
  <li>Struct to Text: CommonGen, DART, E2ENLG, WEBNLG</li>
  <li>Reading Comp:</li>
  <li>Reading Comp w/o commonsensne:</li>
  <li>Conference:</li>
  <li>Misc.:</li>
  <li>Summarization:</li>
  <li>Translation:</li>
</ul>

<h3 id="images">Images</h3>

<h3 id="video">Video</h3>

<p>###</p>

<h2 id="appendix-b-utilities">Appendix B: Utilities</h2>

<p>I provide these utilities to make it as simple as possible to integrate <code class="language-plaintext highlighter-rouge">AEEnv</code> with other libraries.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ae.env.trainsition_fns
ae.env.next_env_fns

ae.trainers.{SAC,RAINBOW,}
ae.executers.{simple,multiagent,}
ae.baselines.

ae.utils.nest.{map,flatten,unflatten,all,any,}
</code></pre></div></div>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    Site modified from <a href="https://github.com/alshedivat/al-folio">Maruan's AI Folio template</a>. Specifically, I filled in all content sections but reused his jekyll project structure.
    
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
