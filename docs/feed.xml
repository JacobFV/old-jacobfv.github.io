<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-10-27T21:32:56-05:00</updated><id>/feed.xml</id><title type="html">Jacob Valdez</title><subtitle>Personal R&amp;D portfolio site
</subtitle><entry><title type="html">Computatrum</title><link href="/blog/computatrum/" rel="alternate" type="text/html" title="Computatrum" /><published>2021-10-19T00:00:00-05:00</published><updated>2021-10-19T00:00:00-05:00</updated><id>/blog/computatrum</id><content type="html" xml:base="/blog/computatrum/">&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=/projects/computatrum/&quot; /&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Multi-Environment Learning</title><link href="/blog/multi-environment-learning/" rel="alternate" type="text/html" title="Multi-Environment Learning" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/multi-environment-learning</id><content type="html" xml:base="/blog/multi-environment-learning/">&lt;p&gt;Multi-environment learning extends the single environment RL paradigm to multiple environments. It’s like multiagent learning – except your only controlling one agent in multiple environments. In this paradigm, the policy $\pi : o_1, o_2, \dots \rightarrow a_1, a_2, \dots$ takes an observation $o_1, o_2, \dots$ from different environments and produces actions $a_1, a_2, \dots$ for all of them simultaneously on each step. You might have a collection loop like:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;observations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;: At first I couldn’t see why anyone would even want to use this paradigm. After all, if your environments are disjoint, learning to stack blocks probabbly won’t make much a difference on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CartPole&lt;/code&gt;. It’s especially unnecesary to observe both environments simultaneously. However, when your problem domain is in the open-ended world, it is helpful to learn intrinsic priors of logic, social skills, or commonsense knowledge from a simpler area and then apply that information to a more complex environment &lt;em&gt;in context&lt;/em&gt;. When we train our policies in sequential cirricula, the information has to flow from the environment to the weights before it can be used in a later environment. With the multi-environment approach however, policies can learn to access information at the exact time needed by a policy. For instance, you could train an agent where one environment is an interactive ImageNet search engine with no reward and the other environment is a classification challenge with delayed decisions allowed (but still regularized to encourage fast response). Giving the policy the ability to pause and search related images would make it more human like and ideally more accurate. I’m sure you can imagine other scenerios where the multi-environment paradigm is beneficial. (Just consider the computational beenfit of only needing to deploy a single large model to interact with dozens of consenting clients.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Greatest advantage&lt;/strong&gt;: Perhaps the greatest benefit of multi-environment learning is that we can use this paradigm to train otherwise-standard RL agents to &lt;em&gt;learn to adapt and generalize in context&lt;/em&gt;. In &lt;em&gt;parallel randomized domain learning&lt;/em&gt;, we might train our agent to explore different variations of the same procedurally generated environment simultaneously. In &lt;em&gt;staggered lifelong learning&lt;/em&gt;, we would present the agent with a cirricula of environments which do not all have the same start and end time. Of course, these kinds of techniques would require a &lt;em&gt;large set of environments or even procedural environment generators&lt;/em&gt;. These environments/procedural generators would need to be extremely diverse, so I’m just going to gather them by hand. Soon I hope to have an RL agent that can find more for me.&lt;/p&gt;</content><author><name></name></author><summary type="html">Multi-environment learning extends the single environment RL paradigm to multiple environments. It’s like multiagent learning – except your only controlling one agent in multiple environments. In this paradigm, the policy $\pi : o_1, o_2, \dots \rightarrow a_1, a_2, \dots$ takes an observation $o_1, o_2, \dots$ from different environments and produces actions $a_1, a_2, \dots$ for all of them simultaneously on each step. You might have a collection loop like: while True: observations = [env.obs for env in envs] actions = policy(observations) for env, action in zip(envs, actions): env.step(action)</summary></entry><entry><title type="html">The API</title><link href="/blog/the-api/" rel="alternate" type="text/html" title="The API" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/the-api</id><content type="html" xml:base="/blog/the-api/">&lt;p&gt;Though diverse, most humans share a common set of input and output modalities: sight, hearing, touch, and skeletal motor control. This common interface aligns our perception of the world to a rasonable degree and facilitates social interaction and collaboration. As progress continues towards developing humanly-impactful foundation models and policies, I propose establishing a common set of modalities for artificial intelligence systems as well: the &lt;strong&gt;anthrocentric policy interface&lt;/strong&gt; or &lt;strong&gt;API&lt;/strong&gt;. The API is not a formal application program interface specification but rather a description of the modalities that can be used to interact with an AI system. Following is my proposal for momdalities included by version 0.0 of the API:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;motor: sparse variable-dimensional graph of continuous motor values (\(x_{motor}, y_{motor} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to produce appropriate outputs as modulated by their reward signals.&lt;/li&gt;
  &lt;li&gt;touch: sparse variable-dimensional graph of continuous touch values (\(x_{touch}, y_{touch} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to output predicted touch values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;reward: single or multidimensional reward signal (vector, \(x_{reward}, y_{reward} \in \mathbb{R}^n, n \in \mathbb{N}\)). Sparse or dense. API-compatible models are expected to output predicted reward value for the next interaction step.&lt;/li&gt;
  &lt;li&gt;text: variable length string of tokens (\(x_{text}, y_{text} \in \mathbb{Z}^t, t \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted text values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;audio: variable length, single or multi-channel waveform (\(x_{audio}, y_{audio} \in [0,1]^{t,c}, t,c \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted audio values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;video: variable length, variable size, variable channel image sequence  (\(x_{video}, y_{video} \in [0,1]^{t,h,w,c}, t \in \mathbb{Z}_{+}, h,w,c \in \mathbb{N}\)). API-compatible models are expected to output predicted video values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;hidden state: arbitrary data structure. Developers can use this to store and retrieve state information between interactions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All modalities exchange input and output information over multiple interaction steps. For example, a vision transformer may recieve top down feedback and attend for multiple steps over a single image, an in-context learning model might recieve several batches of dataset examples over the interaction, or a conversation bot could be force fit on real conversations over time. This demands clarify time:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;training step = number of dataset minibatches or environment episodes trained on&lt;/li&gt;
  &lt;li&gt;interaction step = number of modality inputs and outputs performed; the classical time measure within a single episode&lt;/li&gt;
  &lt;li&gt;modality time step = which frame number in the video sequence; which token index in the text sequence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the client’s perspective, modality inputs and outputs are optional. These will be represented by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; in Python, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;null&lt;/code&gt; in JSON, or similar in other languages. For example, you might only occasionally send images to a multimodal conversation bot but utilize the text modality on every step. Some ‘multimodal’ datasets and environments will only provide information for two or three modalities. Additionally, a transcribed audiovideo dataset might have zero-length text sequences, waveforms, and videos. Not all outputs will be used in every application so they can be explicitly marked  as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do_not_compute&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Handling time steps within each modality introduces many technical challanges. Developer must consider questions as: how should a multimodal video model combine 8KHz audio with 12 FPS video? will the robot controller be presented with 100ms, 6-frame snippets of 60FPS video on each interaction step? or should it just recieve a single frame every 50ms? Should an audio transcriber recieve the full audio track or just 10 second staggered segments on each interaction step? Or should it use its motor modality to deliberately move its perception window around?&lt;/p&gt;

&lt;p&gt;API-compatible models are expected to accommadate arbitrary numbers of each modality at initialization time. For example, you might initialize a policy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vision:left&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vision:right&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:magent&lt;/code&gt; (social communication) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:control&lt;/code&gt; (user directions). However, API-compatible models are not required to accomodate new modalities after initialization (you don’t have to tie the weights; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:magent&lt;/code&gt; preprocessor network can be very different from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:control&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;I imagine a few ways this interface may be used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For N-way classification problems, you might have a motor graph with N 1-dimensional nodes.&lt;/li&gt;
  &lt;li&gt;For N-dimensional regression, you might have a motor graph with just 1 N-dimensional node.&lt;/li&gt;
  &lt;li&gt;For computer vision, the model iteratively attends to the image.&lt;/li&gt;
  &lt;li&gt;For 3D robotic agents, … You get the idea.&lt;/li&gt;
  &lt;li&gt;Managers might ask, ‘Is it compatible with the API?’&lt;/li&gt;
  &lt;li&gt;Researchers might write, ‘this dataset/environment conforms to the API’.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope the API provides a common interface to facilitate community collaboration towards developing increasingly strong and general artificial intelligence systems. My focus for the next few months will be building the Artificial Experience – a dataset of datasets and environments (including procedurally generated ones) that can provide training signals for increasingly general API-compatible agents.&lt;/p&gt;</content><author><name></name></author><summary type="html">Though diverse, most humans share a common set of input and output modalities: sight, hearing, touch, and skeletal motor control. This common interface aligns our perception of the world to a rasonable degree and facilitates social interaction and collaboration. As progress continues towards developing humanly-impactful foundation models and policies, I propose establishing a common set of modalities for artificial intelligence systems as well: the anthrocentric policy interface or API. The API is not a formal application program interface specification but rather a description of the modalities that can be used to interact with an AI system. Following is my proposal for momdalities included by version 0.0 of the API: motor: sparse variable-dimensional graph of continuous motor values (\(x_{motor}, y_{motor} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to produce appropriate outputs as modulated by their reward signals. touch: sparse variable-dimensional graph of continuous touch values (\(x_{touch}, y_{touch} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to output predicted touch values for the next interaction step. reward: single or multidimensional reward signal (vector, \(x_{reward}, y_{reward} \in \mathbb{R}^n, n \in \mathbb{N}\)). Sparse or dense. API-compatible models are expected to output predicted reward value for the next interaction step. text: variable length string of tokens (\(x_{text}, y_{text} \in \mathbb{Z}^t, t \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted text values for the next interaction step. audio: variable length, single or multi-channel waveform (\(x_{audio}, y_{audio} \in [0,1]^{t,c}, t,c \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted audio values for the next interaction step. video: variable length, variable size, variable channel image sequence (\(x_{video}, y_{video} \in [0,1]^{t,h,w,c}, t \in \mathbb{Z}_{+}, h,w,c \in \mathbb{N}\)). API-compatible models are expected to output predicted video values for the next interaction step. hidden state: arbitrary data structure. Developers can use this to store and retrieve state information between interactions.</summary></entry><entry><title type="html">The Node Neural Network (NNN)</title><link href="/blog/the-node-neural-network/" rel="alternate" type="text/html" title="The Node Neural Network (NNN)" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/the-node-neural-network</id><content type="html" xml:base="/blog/the-node-neural-network/">&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=/projects/the-multi-agent-network/&quot; /&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">The Multi Agent Network</title><link href="/blog/the-multi-agent-network/" rel="alternate" type="text/html" title="The Multi Agent Network" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/the-multi-agent-network</id><content type="html" xml:base="/blog/the-multi-agent-network/">&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=/projects/the-multi-agent-network/&quot; /&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">MNIST Classifier Tutorial</title><link href="/blog/mnist-classifier-tutorial/" rel="alternate" type="text/html" title="MNIST Classifier Tutorial" /><published>2021-10-14T00:00:00-05:00</published><updated>2021-10-14T00:00:00-05:00</updated><id>/blog/mnist-classifier-tutorial</id><content type="html" xml:base="/blog/mnist-classifier-tutorial/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/MNIST_Classifier.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">The Artificial Experience</title><link href="/blog/the-artificial-experience/" rel="alternate" type="text/html" title="The Artificial Experience" /><published>2021-10-10T00:00:00-05:00</published><updated>2021-10-10T00:00:00-05:00</updated><id>/blog/the-artificial-experience</id><content type="html" xml:base="/blog/the-artificial-experience/">&lt;p&gt;Our implicit objective in the hypothetical artificial general intelligence is to identify as many dimensions of variation to the underlying data structures that real Intelligence operates on and iterate development around that data. For datasets includes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;domain: natural language, vision, audio, robot, etc.&lt;/li&gt;
  &lt;li&gt;data structure: structured, text, image, video, audio, graph, etc., multimodal&lt;/li&gt;
  &lt;li&gt;data representation: discrete, continuous, categorical, binary, etc.&lt;/li&gt;
  &lt;li&gt;problem: classification, regression, clustering, autoencoding, autoregression, etc., no specified problem type.&lt;/li&gt;
  &lt;li&gt;data augmentations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For environments we might consider:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;simulated/real&lt;/li&gt;
  &lt;li&gt;data representation: discrete, continuous, categorical, binary, etc.&lt;/li&gt;
  &lt;li&gt;single objective/multi-objective/no-objective&lt;/li&gt;
  &lt;li&gt;partially/fully observable&lt;/li&gt;
  &lt;li&gt;markovian/non-markovian&lt;/li&gt;
  &lt;li&gt;single agent/multi-agent&lt;/li&gt;
  &lt;li&gt;for multi-agent: cooperative/competitive/mixed-mode&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’ve listed several datasets and environments in the bottom of this post. Ideally, we should train increasingly general ML systems over all of these variations. Still, our training pipelines are very brittle.&lt;/p&gt;

&lt;p&gt;I propose developing a tool that allows ML praticioners to easily train their agents across many datasets and environments: the Artificial Experience (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ae&lt;/code&gt;). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ae&lt;/code&gt; should provide minially necesary extensions to extend existing open-source dataset loaders, environments, and hubs. It should be agnostic to the actual training paradigm and tricks (augmentations, experience replay, cirriculum learning, etc.) but itegrate cleanly with tools that do. The following is a declarative description of what I plan to make:&lt;/p&gt;

&lt;p&gt;The ArtificialExperience environment (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AEEnv&lt;/code&gt;) provides a wrapper for multiple environments. Datasets may be wrapped into environments. Turn-based multiagent environments are wrapped into parallel agent cycles environments (you can unwrap this later in your multiagent executor). An AEEnv might look like this:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AEEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;DatasetEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tfds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'coco'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# multimodal information
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;DatasetEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'hub://activeloop/mnist-train'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# cloud-native data
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;DatasetEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tfds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'anli'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# quick customization
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'CartPole-v0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# continous observation, discrete control
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Pong-v0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# rgb image, discrete actions
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'HalfCheetah-v2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# continuous observation, continuous control
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;gym_starcraft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;starcraft_base_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# starcraft env
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;pettingzoo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atari&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mario_bros_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# multiagent atari env
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AEEnv&lt;/code&gt; also makes it easy to train on prespecified problem domains with datasets and environments minimally specified by some overlapping hierarchial tag-based system. Not all environments have the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tag&lt;/code&gt; attribute, so those will be ignored. However, the inbuilt list of envionrments should all support this schema. These filters can be changed at any moment between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AEEnv&lt;/code&gt; steps. See Appendix A for a list of what I want to support.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AEEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'domain:text-commonsense'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'domain:image'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'domain:multiagent'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;exclude&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'domain:reward-free-rl'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'domain:multiagent/atari'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'test:True'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# train on text-commonsense (specific), image datasets (broad), and multiagent RL environments (broad) but don't train on the multiagent/atari environment or multiagent environments that don't have a environment specified reward.
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AEEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# train on all inbuilt datasets and environments
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;next_env_fn(last_step_data: step, curr_env: env, available_envs: List[env]) -&amp;gt; env&lt;/code&gt; determines which environment to sample from at &lt;em&gt;each&lt;/em&gt; timestep. This may be a simple ‘wait until all done’s are true’ (for datasets, after all epochs) or it may be a more complex user-designed autocirricula system. An &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;env_transition_fn(old_env: env, new_env: env) -&amp;gt; NoReturn&lt;/code&gt; can be specified to make surface-level model changes when the environment (and hence its interface) changes.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# samples a different environment *at every step*. Simple way to train on a diverse lot of datasets within the same problem domain (like images).
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_env_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_step_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curr_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available_envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;available_envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_env_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_env_fn&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# lazy next_env_fn specification
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# samples a different environment *after every epoch*. Traditional approach to multi-dataset training.
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;next_env_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_step_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curr_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available_envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_step_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curr_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;available_envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curr_env&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AEEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_env_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_env_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# early next_env_fn specification
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# builds a new input and output layer for new environments
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;env_transition_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NoReturn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ae&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# the environments are compatible, no need to change the model
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# the environments are incompatible, we need to change the model
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;build_new_input_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;build_new_output_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env_transition_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env_transition_fn&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# lazy env_transition_fn specification
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# builds a new input and output layer for new environments
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;env_transition_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NoReturn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DatasetEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DatasetEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# the environments are either both datasets or both regular environments, no need to change the model
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# the environments are incompatible, we need to change the training pipeline
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;change_training_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AEEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env_transition_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env_transition_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# early env_transition_fn specification
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Data is presented at each step as an agent-separated dictionary of namedtuple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Step&lt;/code&gt;’s as well as meta information about the environment state (or dataset index). A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Step&lt;/code&gt; is a nested batch of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;observation&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reward&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;done&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;information&lt;/code&gt; . In most cases, these fields will be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt;. For example, datasets do not provide a reward (reward is determined by the training pipeline which is not part of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AEEnv&lt;/code&gt;).For supervised learning datasets, the observations include both X and Y while for unsupervised learning datasets, the observations include only X. Also, most datasets and environments will only present information for a single agent. Here are some examples:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;evalLoop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_supervised&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_step_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;agent_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;act&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent_name&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent_names&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;information&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;appendix-a-datasets-and-environments&quot;&gt;Appendix A: Datasets and environments&lt;/h2&gt;

&lt;p&gt;The categories overlap. For instance, image captioning might be in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt; category, but also in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; category. The high-level hierarchy might be:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;images&lt;/li&gt;
  &lt;li&gt;text&lt;/li&gt;
  &lt;li&gt;video&lt;/li&gt;
  &lt;li&gt;audio
TODO&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nlp&quot;&gt;NLP&lt;/h3&gt;
&lt;p&gt;from Google’s &lt;a href=&quot;https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html&quot;&gt;FLAN blog post&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Natural language inference: ANLI, RTE, CB, SNLI, MNLI, QNLI, WNLI, QNLI,&lt;/li&gt;
  &lt;li&gt;Commonsense: CoPA, HeliaSwag, PiQA, StoryCloze&lt;/li&gt;
  &lt;li&gt;Sentiment: IMDB, Sent140, SST-2, Yelp&lt;/li&gt;
  &lt;li&gt;Paraphrase: MRPC, QQP, PAWS, STS-B&lt;/li&gt;
  &lt;li&gt;Closed book QA: ARC (easy/chal), NQ, TQA&lt;/li&gt;
  &lt;li&gt;Struct to Text: CommonGen, DART, E2ENLG, WEBNLG&lt;/li&gt;
  &lt;li&gt;Reading Comp:&lt;/li&gt;
  &lt;li&gt;Reading Comp w/o commonsensne:&lt;/li&gt;
  &lt;li&gt;Conference:&lt;/li&gt;
  &lt;li&gt;Misc.:&lt;/li&gt;
  &lt;li&gt;Summarization:&lt;/li&gt;
  &lt;li&gt;Translation:&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;images&quot;&gt;Images&lt;/h3&gt;

&lt;h3 id=&quot;video&quot;&gt;Video&lt;/h3&gt;

&lt;p&gt;###&lt;/p&gt;

&lt;h2 id=&quot;appendix-b-utilities&quot;&gt;Appendix B: Utilities&lt;/h2&gt;

&lt;p&gt;I provide these utilities to make it as simple as possible to integrate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AEEnv&lt;/code&gt; with other libraries.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ae.env.trainsition_fns
ae.env.next_env_fns

ae.trainers.{SAC,RAINBOW,}
ae.executers.{simple,multiagent,}
ae.baselines.

ae.utils.nest.{map,flatten,unflatten,all,any,}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">Our implicit objective in the hypothetical artificial general intelligence is to identify as many dimensions of variation to the underlying data structures that real Intelligence operates on and iterate development around that data. For datasets includes: domain: natural language, vision, audio, robot, etc. data structure: structured, text, image, video, audio, graph, etc., multimodal data representation: discrete, continuous, categorical, binary, etc. problem: classification, regression, clustering, autoencoding, autoregression, etc., no specified problem type. data augmentations.</summary></entry><entry><title type="html">Full-Stack Artificial Intelligence</title><link href="/blog/full-stack-artificial-intelligence/" rel="alternate" type="text/html" title="Full-Stack Artificial Intelligence" /><published>2021-10-09T00:00:00-05:00</published><updated>2021-10-09T00:00:00-05:00</updated><id>/blog/full-stack-artificial-intelligence</id><content type="html" xml:base="/blog/full-stack-artificial-intelligence/">&lt;p&gt;We cannot expect the same algorithm to make increasing returns on all problems. The complexity (and ⏱️,💲,🏭) growth of symbolic, deep learning, reinforcement learning, and other AI approaches motivate considering other dimensions of Intelligence. I draw attention to several broad, overlapping dimensions on the evolutionary scale of artificial intelligence:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Intrinsic properties: What are the intrinsic properties of a problem? How should the information flow from problem to solution? How should information flow from inputs to outputs for a single instance of the problem? What priors can we assume?&lt;/li&gt;
  &lt;li&gt;Algorithm design: How do we concretely express the ideal information flow? What computational tricks will help us? How do we choose the right hyper/parameters? How are learnable components (hidden states, parameters, architectures) initialized?&lt;/li&gt;
  &lt;li&gt;Datasets and environments: What data will we need? Does it need to be preprocessed?&lt;/li&gt;
  &lt;li&gt;Training paradigms: How will we impose feedback on the solution space? SL, US, SSL, RL, MARL, multi-paradigm? Should the loss be a weighted sum of losses or should it look more like a pareto curve?&lt;/li&gt;
  &lt;li&gt;Evaluation: How do we evaluate the performance of the solution? What quantitative metrics will we use? What qualitative assessments can we reasonably make?&lt;/li&gt;
  &lt;li&gt;Infrastructure: What are the hardware and software requirements?&lt;/li&gt;
  &lt;li&gt;Human feedback: How will we understand and communicate the system’s performance? What should my iteration speed as the developer be?&lt;/li&gt;
  &lt;li&gt;Existing code: Do I know what’s already been done? What should I do myself?&lt;/li&gt;
  &lt;li&gt;Human capital: What intelligence have others (developers, researchers, other thinkers) already contributed? Could this be a group effort? What motivational forces (free time, interest, project complexity and understandability) should be considered?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I try to trackle these sprawling points in future posts.&lt;/p&gt;</content><author><name></name></author><summary type="html">We cannot expect the same algorithm to make increasing returns on all problems. The complexity (and ⏱️,💲,🏭) growth of symbolic, deep learning, reinforcement learning, and other AI approaches motivate considering other dimensions of Intelligence. I draw attention to several broad, overlapping dimensions on the evolutionary scale of artificial intelligence:</summary></entry><entry><title type="html">Titanic Survival Prediction with TensorFlow</title><link href="/blog/titanic-survival-prediction-with-tensorflow/" rel="alternate" type="text/html" title="Titanic Survival Prediction with TensorFlow" /><published>2021-09-28T00:00:00-05:00</published><updated>2021-09-28T00:00:00-05:00</updated><id>/blog/titanic-survival-prediction-with-tensorflow</id><content type="html" xml:base="/blog/titanic-survival-prediction-with-tensorflow/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/titanic-survival-prediction-with-tensorflow.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Self Organized Criticality</title><link href="/blog/self-organized-criticality/" rel="alternate" type="text/html" title="Self Organized Criticality" /><published>2021-06-22T00:00:00-05:00</published><updated>2021-06-22T00:00:00-05:00</updated><id>/blog/self-organized-criticality</id><content type="html" xml:base="/blog/self-organized-criticality/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/self_organized_criticality.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>