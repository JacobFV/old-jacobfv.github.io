<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-11-30T13:22:54-06:00</updated><id>/feed.xml</id><title type="html">Jacob Valdez</title><subtitle>Personal R&amp;D portfolio site
</subtitle><entry><title type="html">Naive Bayes Classifier</title><link href="/blog/naive-bayes-classifier/" rel="alternate" type="text/html" title="Naive Bayes Classifier" /><published>2021-11-30T00:00:00-06:00</published><updated>2021-11-30T00:00:00-06:00</updated><id>/blog/naive-bayes-classifier</id><content type="html" xml:base="/blog/naive-bayes-classifier/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/naive_bayes.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Regularization</title><link href="/blog/regularization/" rel="alternate" type="text/html" title="Regularization" /><published>2021-11-11T00:00:00-06:00</published><updated>2021-11-11T00:00:00-06:00</updated><id>/blog/regularization</id><content type="html" xml:base="/blog/regularization/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/linear_regression.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Estimating the Critical Mass</title><link href="/blog/estimating-the-critical-mass/" rel="alternate" type="text/html" title="Estimating the Critical Mass" /><published>2021-11-06T00:00:00-05:00</published><updated>2021-11-06T00:00:00-05:00</updated><id>/blog/estimating-the-critical-mass</id><content type="html" xml:base="/blog/estimating-the-critical-mass/">&lt;p&gt;I’m sure somebody has made these kinds of analyses in much greator detail, but I wanted to get a sense of the computational limits that we are presently at. Many of the numbers are pulled out of the internet without serious effort; others are (explicitly) made-up.&lt;/p&gt;

&lt;h2 id=&quot;energy-efficiency&quot;&gt;Energy efficiency&lt;/h2&gt;
&lt;p&gt;At its developed peak, the brain might have 200 trillion synapses and consume 1760 kJ/day. Making this a ratio, we get 200 trillion synapses / 20W = 10 trillion synapses / watt. Suppose 1 synapse performs at least 10 ‘operations’ per second. Then the brain performs at least 100 TFLOPS with an ideal efficiency exceeding 100 TFLOPS/watt. Compare this to machine computation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1911.11313.pdf&quot;&gt;This paper (fig 5)&lt;/a&gt; says that in 2020, GPUs reach 100 GFLOPS/watt. However it notes that energy efficiency is exponentially increasing (rough estimate, 10x every 10 years)&lt;/li&gt;
  &lt;li&gt;The v3-32 TPU Pod delivers ~1680TFLOPS (see below) with &lt;a href=&quot;https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/&quot;&gt;estimated power consumption&lt;/a&gt; 200W/core*32cores = 6.4kW. This makes about 250GFLOPS/watt. Only about 3 orders of magnitude less than this brain estimate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;compute-cost&quot;&gt;Compute cost&lt;/h2&gt;
&lt;p&gt;Suppose it costs $10/day to sustain a human brain. Using the above measures, then the brain can perform at least 2000 trillion operations per second using only $0.0001… for a single second. This makes 17280000 TFLOP/$ or 17 exaflops per dollar. The v3-32 TPU Pod ideally approaches 4 times the single 8 core TPU performance = 4 * 420TFLOPS = 1680TFLOPS at a price $10,512 / month or $0.00400219298 per second. This makes 1680TFLOP/$0.00400219298 or 419769 operations per $ or about 0.5MFLOP/$. digital compute infrastructure looks on the order of 10^18 times less efficient than neuronal computation.&lt;/p&gt;

&lt;h2 id=&quot;raw-compute&quot;&gt;Raw Compute&lt;/h2&gt;
&lt;p&gt;The 100 TFLOPS brain estimate looks rather small.&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m sure somebody has made these kinds of analyses in much greator detail, but I wanted to get a sense of the computational limits that we are presently at. Many of the numbers are pulled out of the internet without serious effort; others are (explicitly) made-up.</summary></entry><entry><title type="html">Image Classification</title><link href="/blog/image-classification/" rel="alternate" type="text/html" title="Image Classification" /><published>2021-11-02T00:00:00-05:00</published><updated>2021-11-02T00:00:00-05:00</updated><id>/blog/image-classification</id><content type="html" xml:base="/blog/image-classification/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/image_classification.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Generalization – Fast and Slow</title><link href="/blog/generalization-fast-and-slow/" rel="alternate" type="text/html" title="Generalization – Fast and Slow" /><published>2021-10-31T00:00:00-05:00</published><updated>2021-10-31T00:00:00-05:00</updated><id>/blog/generalization-fast-and-slow</id><content type="html" xml:base="/blog/generalization-fast-and-slow/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Computatrum</title><link href="/blog/computatrum/" rel="alternate" type="text/html" title="Computatrum" /><published>2021-10-19T00:00:00-05:00</published><updated>2021-10-19T00:00:00-05:00</updated><id>/blog/computatrum</id><content type="html" xml:base="/blog/computatrum/">&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=/projects/computatrum/&quot; /&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Multi-Environment Learning</title><link href="/blog/multi-environment-learning/" rel="alternate" type="text/html" title="Multi-Environment Learning" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/multi-environment-learning</id><content type="html" xml:base="/blog/multi-environment-learning/">&lt;p&gt;Multi-environment learning extends the single environment RL paradigm to multiple environments. It’s like multiagent learning – except your only controlling one agent in multiple environments. In this paradigm, the policy $\pi : o_1, o_2, \dots \rightarrow a_1, a_2, \dots$ takes an observation $o_1, o_2, \dots$ from different environments and produces actions $a_1, a_2, \dots$ for all of them simultaneously on each step. You might have a collection loop like:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;observations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;: At first I couldn’t see why anyone would even want to use this paradigm. After all, if your environments are disjoint, learning to stack blocks probabbly won’t make much a difference on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CartPole&lt;/code&gt;. It’s especially unnecesary to observe both environments simultaneously. However, when your problem domain is in the open-ended world, it is helpful to learn intrinsic priors of logic, social skills, or commonsense knowledge from a simpler area and then apply that information to a more complex environment &lt;em&gt;in context&lt;/em&gt;. When we train our policies in sequential cirricula, the information has to flow from the environment to the weights before it can be used in a later environment. With the multi-environment approach however, policies can learn to access information at the exact time needed by a policy. For instance, you could train an agent where one environment is an interactive ImageNet search engine with no reward and the other environment is a classification challenge with delayed decisions allowed (but still regularized to encourage fast response). Giving the policy the ability to pause and search related images would make it more human like and ideally more accurate. I’m sure you can imagine other scenerios where the multi-environment paradigm is beneficial. (Just consider the computational beenfit of only needing to deploy a single large model to interact with dozens of consenting clients.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Greatest advantage&lt;/strong&gt;: Perhaps the greatest benefit of multi-environment learning is that we can use this paradigm to train otherwise-standard RL agents to &lt;em&gt;learn to adapt and generalize in context&lt;/em&gt;. In &lt;em&gt;parallel randomized domain learning&lt;/em&gt;, we might train our agent to explore different variations of the same procedurally generated environment simultaneously. In &lt;em&gt;staggered lifelong learning&lt;/em&gt;, we would present the agent with a cirricula of environments which do not all have the same start and end time. Of course, these kinds of techniques would require a &lt;em&gt;large set of environments or even procedural environment generators&lt;/em&gt;. These environments/procedural generators would need to be extremely diverse, so I’m just going to gather them by hand. Soon I hope to have an RL agent that can find more for me.&lt;/p&gt;</content><author><name></name></author><summary type="html">Multi-environment learning extends the single environment RL paradigm to multiple environments. It’s like multiagent learning – except your only controlling one agent in multiple environments. In this paradigm, the policy $\pi : o_1, o_2, \dots \rightarrow a_1, a_2, \dots$ takes an observation $o_1, o_2, \dots$ from different environments and produces actions $a_1, a_2, \dots$ for all of them simultaneously on each step. You might have a collection loop like: while True: observations = [env.obs for env in envs] actions = policy(observations) for env, action in zip(envs, actions): env.step(action)</summary></entry><entry><title type="html">The API</title><link href="/blog/the-api/" rel="alternate" type="text/html" title="The API" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/the-api</id><content type="html" xml:base="/blog/the-api/">&lt;p&gt;Though diverse, most humans share a common set of input and output modalities: sight, hearing, touch, and skeletal motor control. This common interface aligns our perception of the world to a rasonable degree and facilitates social interaction and collaboration. As progress continues towards developing humanly-impactful foundation models and policies, I propose establishing a common set of modalities for artificial intelligence systems as well: the &lt;strong&gt;anthrocentric policy interface&lt;/strong&gt; or &lt;strong&gt;API&lt;/strong&gt;. The API is not a formal application program interface specification but rather a description of the modalities that can be used to interact with an AI system. Following is my proposal for momdalities included by version 0.0 of the API:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;motor: sparse variable-dimensional graph of continuous motor values (\(x_{motor}, y_{motor} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to produce appropriate outputs as modulated by their reward signals.&lt;/li&gt;
  &lt;li&gt;touch: sparse variable-dimensional graph of continuous touch values (\(x_{touch}, y_{touch} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to output predicted touch values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;reward: single or multidimensional reward signal (vector, \(x_{reward}, y_{reward} \in \mathbb{R}^n, n \in \mathbb{N}\)). Sparse or dense. API-compatible models are expected to output predicted reward value for the next interaction step.&lt;/li&gt;
  &lt;li&gt;text: variable length string of tokens (\(x_{text}, y_{text} \in \mathbb{Z}^t, t \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted text values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;audio: variable length, single or multi-channel waveform (\(x_{audio}, y_{audio} \in [0,1]^{t,c}, t,c \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted audio values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;video: variable length, variable size, variable channel image sequence  (\(x_{video}, y_{video} \in [0,1]^{t,h,w,c}, t \in \mathbb{Z}_{+}, h,w,c \in \mathbb{N}\)). API-compatible models are expected to output predicted video values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;hidden state: arbitrary data structure. Developers can use this to store and retrieve state information between interactions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All modalities exchange input and output information over multiple interaction steps. For example, a vision transformer may recieve top down feedback and attend for multiple steps over a single image, an in-context learning model might recieve several batches of dataset examples over the interaction, or a conversation bot could be force fit on real conversations over time. This demands clarify time:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;training step = number of dataset minibatches or environment episodes trained on&lt;/li&gt;
  &lt;li&gt;interaction step = number of modality inputs and outputs performed; the classical time measure within a single episode&lt;/li&gt;
  &lt;li&gt;modality time step = which frame number in the video sequence; which token index in the text sequence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the client’s perspective, modality inputs and outputs are optional. These will be represented by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; in Python, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;null&lt;/code&gt; in JSON, or similar in other languages. For example, you might only occasionally send images to a multimodal conversation bot but utilize the text modality on every step. Some ‘multimodal’ datasets and environments will only provide information for two or three modalities. Additionally, a transcribed audiovideo dataset might have zero-length text sequences, waveforms, and videos. Not all outputs will be used in every application so they can be explicitly marked  as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do_not_compute&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Handling time steps within each modality introduces many technical challanges. Developer must consider questions as: how should a multimodal video model combine 8KHz audio with 12 FPS video? will the robot controller be presented with 100ms, 6-frame snippets of 60FPS video on each interaction step? or should it just recieve a single frame every 50ms? Should an audio transcriber recieve the full audio track or just 10 second staggered segments on each interaction step? Or should it use its motor modality to deliberately move its perception window around?&lt;/p&gt;

&lt;p&gt;API-compatible models are expected to accommadate arbitrary numbers of each modality at initialization time. For example, you might initialize a policy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vision:left&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vision:right&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:magent&lt;/code&gt; (social communication) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:control&lt;/code&gt; (user directions). However, API-compatible models are not required to accomodate new modalities after initialization (you don’t have to tie the weights; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:magent&lt;/code&gt; preprocessor network can be very different from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:control&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;I imagine a few ways this interface may be used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For N-way classification problems, you might have a motor graph with N 1-dimensional nodes.&lt;/li&gt;
  &lt;li&gt;For N-dimensional regression, you might have a motor graph with just 1 N-dimensional node.&lt;/li&gt;
  &lt;li&gt;For computer vision, the model iteratively attends to the image.&lt;/li&gt;
  &lt;li&gt;For 3D robotic agents, … You get the idea.&lt;/li&gt;
  &lt;li&gt;Managers might ask, ‘Is it compatible with the API?’&lt;/li&gt;
  &lt;li&gt;Researchers might write, ‘this dataset/environment conforms to the API’.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope the API provides a common interface to facilitate community collaboration towards developing increasingly strong and general artificial intelligence systems. My focus for the next few months will be building the Artificial Experience – a dataset of datasets and environments (including procedurally generated ones) that can provide training signals for increasingly general API-compatible agents.&lt;/p&gt;</content><author><name></name></author><summary type="html">Though diverse, most humans share a common set of input and output modalities: sight, hearing, touch, and skeletal motor control. This common interface aligns our perception of the world to a rasonable degree and facilitates social interaction and collaboration. As progress continues towards developing humanly-impactful foundation models and policies, I propose establishing a common set of modalities for artificial intelligence systems as well: the anthrocentric policy interface or API. The API is not a formal application program interface specification but rather a description of the modalities that can be used to interact with an AI system. Following is my proposal for momdalities included by version 0.0 of the API: motor: sparse variable-dimensional graph of continuous motor values (\(x_{motor}, y_{motor} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to produce appropriate outputs as modulated by their reward signals. touch: sparse variable-dimensional graph of continuous touch values (\(x_{touch}, y_{touch} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to output predicted touch values for the next interaction step. reward: single or multidimensional reward signal (vector, \(x_{reward}, y_{reward} \in \mathbb{R}^n, n \in \mathbb{N}\)). Sparse or dense. API-compatible models are expected to output predicted reward value for the next interaction step. text: variable length string of tokens (\(x_{text}, y_{text} \in \mathbb{Z}^t, t \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted text values for the next interaction step. audio: variable length, single or multi-channel waveform (\(x_{audio}, y_{audio} \in [0,1]^{t,c}, t,c \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted audio values for the next interaction step. video: variable length, variable size, variable channel image sequence (\(x_{video}, y_{video} \in [0,1]^{t,h,w,c}, t \in \mathbb{Z}_{+}, h,w,c \in \mathbb{N}\)). API-compatible models are expected to output predicted video values for the next interaction step. hidden state: arbitrary data structure. Developers can use this to store and retrieve state information between interactions.</summary></entry><entry><title type="html">The Node Neural Network (NNN)</title><link href="/blog/the-node-neural-network/" rel="alternate" type="text/html" title="The Node Neural Network (NNN)" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/the-node-neural-network</id><content type="html" xml:base="/blog/the-node-neural-network/">&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=/projects/the-multi-agent-network/&quot; /&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">The Multi Agent Network</title><link href="/blog/the-multi-agent-network/" rel="alternate" type="text/html" title="The Multi Agent Network" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/the-multi-agent-network</id><content type="html" xml:base="/blog/the-multi-agent-network/">&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=/projects/the-multi-agent-network/&quot; /&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>