<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-12-06T14:41:18-06:00</updated><id>/feed.xml</id><title type="html">Jacob Valdez</title><subtitle>Personal R&amp;D portfolio site
</subtitle><entry><title type="html">Design Patterns for AI</title><link href="/blog/design-patterns-for-ai/" rel="alternate" type="text/html" title="Design Patterns for AI" /><published>2021-12-06T00:00:00-06:00</published><updated>2021-12-06T00:00:00-06:00</updated><id>/blog/design-patterns-for-ai</id><content type="html" xml:base="/blog/design-patterns-for-ai/">&lt;p&gt;Engineering generally starts off with a few high-level abstract ambitions that are refined and translated with increasing clarity into physical realizations. In software engineering, the outputs of this process are unambiguous executable statements. Of course, the whole process is iterative with loops of increasing frequency over the project lifespan and tools automating this optimization (compiler chains, code optimization, high-level languages) operating at progressively higher-levels further reducing the idea to implementation transit time. When neural networks are the focus of software engineering, translation from informal to unambiguous specification is almost immediate (given sufficient understanding of programming, math, deep learning, and thermodynamics) and the brunt of effort shifts (as it rightly should) to actually testing deep learning hypotheses. In the problem space of ‘human-level’ or ‘general’ intelligence, this demands attempting to consolidate as many ideas in neuroscience, cognitive psychology, and artificial intelligence as possible into a working system implementation. While this consolidation appears deceptively simple in literature, the constituent ideas are often lost in implementation&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Therefore, I explicitly list several below to help keep in mind some desired cognitive dynamics of increasingly general intelligence.&lt;/p&gt;

&lt;p&gt;TODO: list all my ideas here. citations are allowed, but focus on getting ideas out.
&lt;a href=&quot;https://link.springer.com/book/10.1007/978-3-540-73267-9&quot;&gt;“Neurodynamics of Cognition and Consciousness”&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At the implementation-end, machine learning research has acquired a diverse array of tools and techniques which represent orders-of-magnitude learning efficiency gains over independent effort. Ever striving to maximize the efficiency with which meaningful information can be infused into a system recommends riding on the energy-momentum of hundreds of thousands of researchers (and zillions of servers). I.E.: incorporating permutations of as many ML research outcomes as possible: pretrained models, individual weights, architectures, training paradigms, existing datasets, and environments all guided by machine and human intuition. Here are some notable ones:&lt;/p&gt;

&lt;p&gt;TODO: list everything I plan to use here&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use neural networks. They are isomorphic to so many computer science concepts allowing reuse of decade of research on graphs (weight matrices can be viewed as an adjacency matrix), MDP’s, continuous functions. More formal cognitive dynamics may be regularized into an inner system-II process run by a system-I neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I know all the cuts to make before I enter the shop, but sawing for 2 hours leaves me exhausted and I end up incorrectly measuring or cutting material unless things are marked correctly. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">Engineering generally starts off with a few high-level abstract ambitions that are refined and translated with increasing clarity into physical realizations. In software engineering, the outputs of this process are unambiguous executable statements. Of course, the whole process is iterative with loops of increasing frequency over the project lifespan and tools automating this optimization (compiler chains, code optimization, high-level languages) operating at progressively higher-levels further reducing the idea to implementation transit time. When neural networks are the focus of software engineering, translation from informal to unambiguous specification is almost immediate (given sufficient understanding of programming, math, deep learning, and thermodynamics) and the brunt of effort shifts (as it rightly should) to actually testing deep learning hypotheses. In the problem space of ‘human-level’ or ‘general’ intelligence, this demands attempting to consolidate as many ideas in neuroscience, cognitive psychology, and artificial intelligence as possible into a working system implementation. While this consolidation appears deceptively simple in literature, the constituent ideas are often lost in implementation1. Therefore, I explicitly list several below to help keep in mind some desired cognitive dynamics of increasingly general intelligence. I know all the cuts to make before I enter the shop, but sawing for 2 hours leaves me exhausted and I end up incorrectly measuring or cutting material unless things are marked correctly. &amp;#8617;</summary></entry><entry><title type="html">Notes on Neuroscience and AI</title><link href="/blog/notes-on-neuroscience-and-ai/" rel="alternate" type="text/html" title="Notes on Neuroscience and AI" /><published>2021-12-03T00:00:00-06:00</published><updated>2021-12-03T00:00:00-06:00</updated><id>/blog/notes-on-neuroscience-and-ai</id><content type="html" xml:base="/blog/notes-on-neuroscience-and-ai/">&lt;p&gt;&lt;a href=&quot;https://link.springer.com/book/10.1007/978-3-540-73267-9&quot;&gt;“Neurodynamics of Cognition and Consciousness”&lt;/a&gt; preface:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The main theme os this volume is the dynamics of higher cognitive functions. The authors of this volume provide a line of arguments explaining why dynamics plays [a] central role in intelligence in biological brains and in man made artifacts[.] Researcher too often make the mistake of identifying intelligence with the projection of intelligence into a brilliant symbolic form, whereas intelligence is the unity of the path (or dynamic trajectory) leading to the observed formalisms and the intermitted appearance of the formalism itself. Intelligence can be understood only in the context of the complementary nature as it is describes by Kelso and collaborators. Neurodynamics had a peculiar property described as the “edge of stability” or “metastability.” Accordingly, the brain as a complete dynamic system is in perpetual movement from one state to another. When the brain reaches a dominant state, it does not [rest] there, rather it immediately moves on and decays into an unordered state, only to emerge a moment later to another prominent state. Freeman has identified neurophysiologic correlates of this metastable wandering along the landscape of brain dynamics in terms of spatio-temporal patterns of oscillations, sudden jumps or phase transitions of local field potentials.
This book explores various aspects of the neurodynamics of metastable cognitive states. It covers a wide range of research areas relates to dynamics of cognition including experimental studies, dynamical modeling and interpretation of cognitive experiments, and theoretical approaches. Spatio-temporal structures of neural activity and synchronization are manifested as propagating phase cones or phase boundaries over the cortex. Methods to detect, identify and characterize such transient structures are described. Identification fo transients is a very hard pattern recognition problem as the simultaneous overlapping dynamical processes provide a noisy and cluttered domain Advanced techniques of dynamical logic progress from vague concepts to increasingly crisp and articulate forms, which is a promising approach to detect the required complex spatio-temporal correlates of cognitive functions. SIgnificant part of the volume is devotes to the description of various components of the actin-perception cycles and sensory processing domains, from cellular to system levels, and applications in intelligence designs&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2105.07284.pdf&quot;&gt;A brain basis of dynamical intelligence for AI and computational neuroscience&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To motivate a brain basis of neural computation, we present a &lt;strong&gt;dynamical view of intelligence&lt;/strong&gt; from which we elaborate concepts of sparsity in network structure, temporal dynamics, and interactive learning. In particular, we suggest that temporal dynamics, as expressed through &lt;strong&gt;neural synchrony&lt;/strong&gt;, &lt;strong&gt;nested oscillations&lt;/strong&gt;, and &lt;strong&gt;flexible sequences&lt;/strong&gt;, provide a rich computational layer for reading and updating hierarchical models distributed in long-term memory networks. &lt;em&gt;(p. 1)&lt;/em&gt;
While it is difficult to answer “What is intelligence?”, it is almost as useful to answer “What is intelligence for?”: Intelligence is for adaptive behavior. Otherwise, an organism would have been better off (as in the neuromythology surrounding the sea squirt) ingesting its brain and attaching itself to a rock. &lt;strong&gt;A corresponding yardstick for intelligence would be the degree to which an organism or agent controls its environment in service of continued survival&lt;/strong&gt;. Indeed, &lt;strong&gt;extending this assessment to novel or unpredicted situations, along ecological dimensions, should correlate with generalized problem-solving capacity&lt;/strong&gt;. &lt;em&gt;(p. 2)&lt;/em&gt;
This not-unusual definition of intelligence puts AI (based on disembodied and nonagentic neural nets trained on datasets lacking spatial, temporal, epistemic, mnemonic, social, and/or environmental context) at a disadvantage for purposes beyond hypercompetent regression and classification. Behavior is variable and complex, but it is also hierarchically organized through time in all animals, with &lt;strong&gt;humans exhibiting perhaps the deepest such hierarchies&lt;/strong&gt;. Conceptual knowledge is similarly hierarchical and demanding of flexibility, reconfigurability, and combinatoric expressiveness (cf. the compositionality and systematicity of language). High level cognition is ordered, temporal, and dynamical in that &lt;strong&gt;what came before conditions the &lt;em&gt;meaning&lt;/em&gt; of what comes after, with lifelong horizons in both directions&lt;/strong&gt; &lt;em&gt;(p. 2)&lt;/em&gt;
Typically &amp;lt;1–2% of possible unit-wise connections exist within the cortico-limbic circuits of the hippocampus and neocortex. The impressive combinatorics inherent in this level of sparsity give rise to the intuitive, but perhaps wishful, notion that discovering the underlying motifs, generating functions, or connectomes of synaptic connectivity will unlock the brain’s neural coding secrets. Without such sparsity, dense connectivity either reliably relaxes into pattern completion for recurrent models viz. Hopfield nets, or universal function approximation for feedforward models viz. multi-layer perceptrons and deep learning. &lt;strong&gt;Brains appear to do both, but also much more.&lt;/strong&gt; Density, as in typical artificial neural nets (ANNs), collapses the space of possible network configurations to that of size and layer architecture. Having far fewer degrees-of-freedom greatly restricts structural, and thus functional, diversity. &lt;em&gt;(p. 4)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&quot;&gt;The Bitter Lesson&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach.
[…]
The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf&quot;&gt;Pruning neural networks without any data by iteratively conserving synaptic flow&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Recent works have identified, through an expensive sequence of training
and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an affirmative answer to this question through theory driven algorithm design&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://research.fb.com/wp-content/uploads/2021/11/Parameter-Prediction-for-Unseen-Deep-Architectures.pdf&quot;&gt;Parameter Prediction for Unseen Deep Architectures&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks
approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">“Neurodynamics of Cognition and Consciousness” preface:</summary></entry><entry><title type="html">Naive Bayes Classifier</title><link href="/blog/naive-bayes-classifier/" rel="alternate" type="text/html" title="Naive Bayes Classifier" /><published>2021-11-30T00:00:00-06:00</published><updated>2021-11-30T00:00:00-06:00</updated><id>/blog/naive-bayes-classifier</id><content type="html" xml:base="/blog/naive-bayes-classifier/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/naive_bayes.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Regularization</title><link href="/blog/regularization/" rel="alternate" type="text/html" title="Regularization" /><published>2021-11-11T00:00:00-06:00</published><updated>2021-11-11T00:00:00-06:00</updated><id>/blog/regularization</id><content type="html" xml:base="/blog/regularization/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/linear_regression.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Estimating the Critical Mass</title><link href="/blog/estimating-the-critical-mass/" rel="alternate" type="text/html" title="Estimating the Critical Mass" /><published>2021-11-06T00:00:00-05:00</published><updated>2021-11-06T00:00:00-05:00</updated><id>/blog/estimating-the-critical-mass</id><content type="html" xml:base="/blog/estimating-the-critical-mass/">&lt;p&gt;I’m sure somebody has made these kinds of analyses in much greator detail, but I wanted to get a sense of the computational limits that we are presently at. Many of the numbers are pulled out of the internet without serious effort; others are (explicitly) made-up.&lt;/p&gt;

&lt;h2 id=&quot;energy-efficiency&quot;&gt;Energy efficiency&lt;/h2&gt;
&lt;p&gt;At its developed peak, the brain might have 200 trillion synapses and consume 1760 kJ/day. Making this a ratio, we get 200 trillion synapses / 20W = 10 trillion synapses / watt. Suppose 1 synapse performs at least 10 ‘operations’ per second. Then the brain performs at least 100 TFLOPS with an ideal efficiency exceeding 100 TFLOPS/watt. Compare this to machine computation:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1911.11313.pdf&quot;&gt;This paper (fig 5)&lt;/a&gt; says that in 2020, GPUs reach 100 GFLOPS/watt. However it notes that energy efficiency is exponentially increasing (rough estimate, 10x every 10 years)&lt;/li&gt;
  &lt;li&gt;The v3-32 TPU Pod delivers ~1680TFLOPS (see below) with &lt;a href=&quot;https://www.nextplatform.com/2018/05/10/tearing-apart-googles-tpu-3-0-ai-coprocessor/&quot;&gt;estimated power consumption&lt;/a&gt; 200W/core*32cores = 6.4kW. This makes about 250GFLOPS/watt. Only about 3 orders of magnitude less than this brain estimate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;compute-cost&quot;&gt;Compute cost&lt;/h2&gt;
&lt;p&gt;Suppose it costs $10/day to sustain a human brain. Using the above measures, then the brain can perform at least 2000 trillion operations per second using only $0.0001… for a single second. This makes 17280000 TFLOP/$ or 17 exaflops per dollar. The v3-32 TPU Pod ideally approaches 4 times the single 8 core TPU performance = 4 * 420TFLOPS = 1680TFLOPS at a price $10,512 / month or $0.00400219298 per second. This makes 1680TFLOP/$0.00400219298 or 419769 operations per $ or about 0.5MFLOP/$. digital compute infrastructure looks on the order of 10^18 times less efficient than neuronal computation.&lt;/p&gt;

&lt;h2 id=&quot;raw-compute&quot;&gt;Raw Compute&lt;/h2&gt;
&lt;p&gt;The 100 TFLOPS brain estimate looks rather small.&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m sure somebody has made these kinds of analyses in much greator detail, but I wanted to get a sense of the computational limits that we are presently at. Many of the numbers are pulled out of the internet without serious effort; others are (explicitly) made-up.</summary></entry><entry><title type="html">Image Classification</title><link href="/blog/image-classification/" rel="alternate" type="text/html" title="Image Classification" /><published>2021-11-02T00:00:00-05:00</published><updated>2021-11-02T00:00:00-05:00</updated><id>/blog/image-classification</id><content type="html" xml:base="/blog/image-classification/">&lt;div class=&quot;jupyter-notebook&quot; style=&quot;position: relative; width: 100%; margin: 0 auto;&quot;&gt;
  &lt;div class=&quot;jupyter-notebook-iframe-container&quot;&gt;
    &lt;iframe src=&quot;../../notebooks/image_classification.ipynb.html&quot; style=&quot;position: absolute; top: 0; left: 0; border-style: none;&quot; width=&quot;100%&quot; height=&quot;100%&quot; onload=&quot;this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Generalization – Fast and Slow</title><link href="/blog/generalization-fast-and-slow/" rel="alternate" type="text/html" title="Generalization – Fast and Slow" /><published>2021-10-31T00:00:00-05:00</published><updated>2021-10-31T00:00:00-05:00</updated><id>/blog/generalization-fast-and-slow</id><content type="html" xml:base="/blog/generalization-fast-and-slow/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Computatrum</title><link href="/blog/computatrum/" rel="alternate" type="text/html" title="Computatrum" /><published>2021-10-19T00:00:00-05:00</published><updated>2021-10-19T00:00:00-05:00</updated><id>/blog/computatrum</id><content type="html" xml:base="/blog/computatrum/">&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=/projects/computatrum/&quot; /&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Multi-Environment Learning</title><link href="/blog/multi-environment-learning/" rel="alternate" type="text/html" title="Multi-Environment Learning" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/multi-environment-learning</id><content type="html" xml:base="/blog/multi-environment-learning/">&lt;p&gt;Multi-environment learning extends the single environment RL paradigm to multiple environments. It’s like multiagent learning – except your only controlling one agent in multiple environments. In this paradigm, the policy $\pi : o_1, o_2, \dots \rightarrow a_1, a_2, \dots$ takes an observation $o_1, o_2, \dots$ from different environments and produces actions $a_1, a_2, \dots$ for all of them simultaneously on each step. You might have a collection loop like:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;observations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;envs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;: At first I couldn’t see why anyone would even want to use this paradigm. After all, if your environments are disjoint, learning to stack blocks probabbly won’t make much a difference on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CartPole&lt;/code&gt;. It’s especially unnecesary to observe both environments simultaneously. However, when your problem domain is in the open-ended world, it is helpful to learn intrinsic priors of logic, social skills, or commonsense knowledge from a simpler area and then apply that information to a more complex environment &lt;em&gt;in context&lt;/em&gt;. When we train our policies in sequential cirricula, the information has to flow from the environment to the weights before it can be used in a later environment. With the multi-environment approach however, policies can learn to access information at the exact time needed by a policy. For instance, you could train an agent where one environment is an interactive ImageNet search engine with no reward and the other environment is a classification challenge with delayed decisions allowed (but still regularized to encourage fast response). Giving the policy the ability to pause and search related images would make it more human like and ideally more accurate. I’m sure you can imagine other scenerios where the multi-environment paradigm is beneficial. (Just consider the computational beenfit of only needing to deploy a single large model to interact with dozens of consenting clients.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Greatest advantage&lt;/strong&gt;: Perhaps the greatest benefit of multi-environment learning is that we can use this paradigm to train otherwise-standard RL agents to &lt;em&gt;learn to adapt and generalize in context&lt;/em&gt;. In &lt;em&gt;parallel randomized domain learning&lt;/em&gt;, we might train our agent to explore different variations of the same procedurally generated environment simultaneously. In &lt;em&gt;staggered lifelong learning&lt;/em&gt;, we would present the agent with a cirricula of environments which do not all have the same start and end time. Of course, these kinds of techniques would require a &lt;em&gt;large set of environments or even procedural environment generators&lt;/em&gt;. These environments/procedural generators would need to be extremely diverse, so I’m just going to gather them by hand. Soon I hope to have an RL agent that can find more for me.&lt;/p&gt;</content><author><name></name></author><summary type="html">Multi-environment learning extends the single environment RL paradigm to multiple environments. It’s like multiagent learning – except your only controlling one agent in multiple environments. In this paradigm, the policy $\pi : o_1, o_2, \dots \rightarrow a_1, a_2, \dots$ takes an observation $o_1, o_2, \dots$ from different environments and produces actions $a_1, a_2, \dots$ for all of them simultaneously on each step. You might have a collection loop like: while True: observations = [env.obs for env in envs] actions = policy(observations) for env, action in zip(envs, actions): env.step(action)</summary></entry><entry><title type="html">The API</title><link href="/blog/the-api/" rel="alternate" type="text/html" title="The API" /><published>2021-10-18T00:00:00-05:00</published><updated>2021-10-18T00:00:00-05:00</updated><id>/blog/the-api</id><content type="html" xml:base="/blog/the-api/">&lt;p&gt;Though diverse, most humans share a common set of input and output modalities: sight, hearing, touch, and skeletal motor control. This common interface aligns our perception of the world to a rasonable degree and facilitates social interaction and collaboration. As progress continues towards developing humanly-impactful foundation models and policies, I propose establishing a common set of modalities for artificial intelligence systems as well: the &lt;strong&gt;anthrocentric policy interface&lt;/strong&gt; or &lt;strong&gt;API&lt;/strong&gt;. The API is not a formal application program interface specification but rather a description of the modalities that can be used to interact with an AI system. Following is my proposal for momdalities included by version 0.0 of the API:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;motor: sparse variable-dimensional graph of continuous motor values (\(x_{motor}, y_{motor} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to produce appropriate outputs as modulated by their reward signals.&lt;/li&gt;
  &lt;li&gt;touch: sparse variable-dimensional graph of continuous touch values (\(x_{touch}, y_{touch} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to output predicted touch values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;reward: single or multidimensional reward signal (vector, \(x_{reward}, y_{reward} \in \mathbb{R}^n, n \in \mathbb{N}\)). Sparse or dense. API-compatible models are expected to output predicted reward value for the next interaction step.&lt;/li&gt;
  &lt;li&gt;text: variable length string of tokens (\(x_{text}, y_{text} \in \mathbb{Z}^t, t \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted text values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;audio: variable length, single or multi-channel waveform (\(x_{audio}, y_{audio} \in [0,1]^{t,c}, t,c \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted audio values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;video: variable length, variable size, variable channel image sequence  (\(x_{video}, y_{video} \in [0,1]^{t,h,w,c}, t \in \mathbb{Z}_{+}, h,w,c \in \mathbb{N}\)). API-compatible models are expected to output predicted video values for the next interaction step.&lt;/li&gt;
  &lt;li&gt;hidden state: arbitrary data structure. Developers can use this to store and retrieve state information between interactions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All modalities exchange input and output information over multiple interaction steps. For example, a vision transformer may recieve top down feedback and attend for multiple steps over a single image, an in-context learning model might recieve several batches of dataset examples over the interaction, or a conversation bot could be force fit on real conversations over time. This demands clarify time:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;training step = number of dataset minibatches or environment episodes trained on&lt;/li&gt;
  &lt;li&gt;interaction step = number of modality inputs and outputs performed; the classical time measure within a single episode&lt;/li&gt;
  &lt;li&gt;modality time step = which frame number in the video sequence; which token index in the text sequence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the client’s perspective, modality inputs and outputs are optional. These will be represented by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; in Python, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;null&lt;/code&gt; in JSON, or similar in other languages. For example, you might only occasionally send images to a multimodal conversation bot but utilize the text modality on every step. Some ‘multimodal’ datasets and environments will only provide information for two or three modalities. Additionally, a transcribed audiovideo dataset might have zero-length text sequences, waveforms, and videos. Not all outputs will be used in every application so they can be explicitly marked  as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do_not_compute&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Handling time steps within each modality introduces many technical challanges. Developer must consider questions as: how should a multimodal video model combine 8KHz audio with 12 FPS video? will the robot controller be presented with 100ms, 6-frame snippets of 60FPS video on each interaction step? or should it just recieve a single frame every 50ms? Should an audio transcriber recieve the full audio track or just 10 second staggered segments on each interaction step? Or should it use its motor modality to deliberately move its perception window around?&lt;/p&gt;

&lt;p&gt;API-compatible models are expected to accommadate arbitrary numbers of each modality at initialization time. For example, you might initialize a policy with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vision:left&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vision:right&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:magent&lt;/code&gt; (social communication) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:control&lt;/code&gt; (user directions). However, API-compatible models are not required to accomodate new modalities after initialization (you don’t have to tie the weights; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:magent&lt;/code&gt; preprocessor network can be very different from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text:control&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;I imagine a few ways this interface may be used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For N-way classification problems, you might have a motor graph with N 1-dimensional nodes.&lt;/li&gt;
  &lt;li&gt;For N-dimensional regression, you might have a motor graph with just 1 N-dimensional node.&lt;/li&gt;
  &lt;li&gt;For computer vision, the model iteratively attends to the image.&lt;/li&gt;
  &lt;li&gt;For 3D robotic agents, … You get the idea.&lt;/li&gt;
  &lt;li&gt;Managers might ask, ‘Is it compatible with the API?’&lt;/li&gt;
  &lt;li&gt;Researchers might write, ‘this dataset/environment conforms to the API’.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope the API provides a common interface to facilitate community collaboration towards developing increasingly strong and general artificial intelligence systems. My focus for the next few months will be building the Artificial Experience – a dataset of datasets and environments (including procedurally generated ones) that can provide training signals for increasingly general API-compatible agents.&lt;/p&gt;</content><author><name></name></author><summary type="html">Though diverse, most humans share a common set of input and output modalities: sight, hearing, touch, and skeletal motor control. This common interface aligns our perception of the world to a rasonable degree and facilitates social interaction and collaboration. As progress continues towards developing humanly-impactful foundation models and policies, I propose establishing a common set of modalities for artificial intelligence systems as well: the anthrocentric policy interface or API. The API is not a formal application program interface specification but rather a description of the modalities that can be used to interact with an AI system. Following is my proposal for momdalities included by version 0.0 of the API: motor: sparse variable-dimensional graph of continuous motor values (\(x_{motor}, y_{motor} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to produce appropriate outputs as modulated by their reward signals. touch: sparse variable-dimensional graph of continuous touch values (\(x_{touch}, y_{touch} = (V,E), V = \{ v_i : v_i \in \mathbb{R}^{d_{v_i}} \}, E = \{ (v_{src}, v_{dst}) : v_{src}, v_{dst} \in V \}\)). API-compatible models are expected to output predicted touch values for the next interaction step. reward: single or multidimensional reward signal (vector, \(x_{reward}, y_{reward} \in \mathbb{R}^n, n \in \mathbb{N}\)). Sparse or dense. API-compatible models are expected to output predicted reward value for the next interaction step. text: variable length string of tokens (\(x_{text}, y_{text} \in \mathbb{Z}^t, t \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted text values for the next interaction step. audio: variable length, single or multi-channel waveform (\(x_{audio}, y_{audio} \in [0,1]^{t,c}, t,c \in \mathbb{Z}_{+}\)). API-compatible models are expected to output predicted audio values for the next interaction step. video: variable length, variable size, variable channel image sequence (\(x_{video}, y_{video} \in [0,1]^{t,h,w,c}, t \in \mathbb{Z}_{+}, h,w,c \in \mathbb{N}\)). API-compatible models are expected to output predicted video values for the next interaction step. hidden state: arbitrary data structure. Developers can use this to store and retrieve state information between interactions.</summary></entry></feed>